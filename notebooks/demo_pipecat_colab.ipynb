{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤ Agent Vocal IA avec RAG Agentique + Pipecat\n",
    "## 100% Local | Streaming Temps RÃ©el | Google Colab\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‹ Vue d'ensemble du projet\n",
    "\n",
    "Ce notebook prÃ©sente un **agent vocal IA intelligent** utilisant **Pipecat** pour le streaming temps rÃ©el :\n",
    "\n",
    "- ğŸ™ï¸ **Reconnaissance vocale** (Whisper via faster-whisper)\n",
    "- ğŸ¤– **LLM local streaming** (Ollama - Qwen2 1.5B)\n",
    "- ğŸ“š **RAG Agentique** (ChromaDB/FAISS avec routing)\n",
    "- ğŸ”Š **SynthÃ¨se vocale** (Piper TTS franÃ§ais)\n",
    "- âš¡ **Pipeline Pipecat** (latence <2s)\n",
    "\n",
    "### ğŸ¯ Domaines supportÃ©s\n",
    "- **ğŸ”¢ MathÃ©matiques** : Ã‰quations, algÃ¨bre, gÃ©omÃ©trie\n",
    "- **âš›ï¸ Physique** : MÃ©canique, lois de Newton\n",
    "- **ğŸ‡¬ğŸ‡§ Anglais** : Grammaire, temps verbaux\n",
    "\n",
    "### ğŸ—ï¸ Architecture Pipecat\n",
    "```\n",
    "Audio Input â†’ LocalSTTService â†’ RAGService â†’ LocalLLMService â†’ LocalTTSService â†’ Audio Output\n",
    "              (Whisper)        (Router+RAG)  (Ollama)         (Piper)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**âš ï¸ IMPORTANT** : Activez le GPU :\n",
    "- `Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU (T4)`\n",
    "\n",
    "**ğŸ“Š Temps d'installation** : ~10-12 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Ã‰tape 1 : VÃ©rification GPU et Installation des DÃ©pendances\n",
    "\n",
    "Installation de tous les packages nÃ©cessaires (~5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"ğŸš€ VÃ©rification de l'environnement...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# VÃ©rifier GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv\n",
    "\n",
    "print(\"\\nâœ… GPU dÃ©tectÃ© !\")\n",
    "print(\"\\nğŸ“¦ Installation des dÃ©pendances Python...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Fix opentelemetry conflicts first\n",
    "print(\"\\nğŸ”§ RÃ©solution des conflits de dÃ©pendances...\")\n",
    "!pip install -q --upgrade opentelemetry-api==1.37.0 opentelemetry-sdk==1.37.0 opentelemetry-proto==1.37.0\n",
    "\n",
    "# Install core dependencies\n",
    "print(\"\\nğŸ“¦ Installation de Pipecat et dÃ©pendances...\")\n",
    "!pip install -q pipecat-ai[silero]\n",
    "!pip install -q faster-whisper\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q chromadb faiss-cpu\n",
    "!pip install -q piper-tts\n",
    "!pip install -q gradio\n",
    "!pip install -q soundfile librosa\n",
    "\n",
    "print(\"\\nâœ… Toutes les dÃ©pendances Python sont installÃ©es !\")\n",
    "print(\"â„¹ï¸  Les warnings opentelemetry sont normaux et n'affectent pas notre agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Ã‰tape 2 : Installation et DÃ©marrage d'Ollama\n",
    "\n",
    "Ollama est nÃ©cessaire pour exÃ©cuter le LLM localement (~3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¤– Installation d'Ollama...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "print(\"\\nâœ… Ollama installÃ© !\")\n",
    "\n",
    "# Start Ollama server in background\n",
    "print(\"\\nğŸš€ DÃ©marrage du serveur Ollama...\")\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start ollama serve in background\n",
    "ollama_process = subprocess.Popen(['ollama', 'serve'], \n",
    "                                   stdout=subprocess.DEVNULL, \n",
    "                                   stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"âœ… Serveur Ollama dÃ©marrÃ© !\")\n",
    "\n",
    "# Pull the model\n",
    "print(\"\\nğŸ“¥ TÃ©lÃ©chargement du modÃ¨le Qwen2 1.5B (optimisÃ© pour Colab)...\")\n",
    "print(\"â³ Cela peut prendre 2-3 minutes...\")\n",
    "!ollama pull qwen2:1.5b\n",
    "\n",
    "print(\"\\nâœ… ModÃ¨le Qwen2 1.5B prÃªt !\")\n",
    "\n",
    "# Verify\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Ã‰tape 3 : Clonage du Repository et Setup\n",
    "\n",
    "Clonage du code source depuis GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ“‚ Clonage du repository...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Clone repository\n",
    "repo_url = \"https://github.com/Romainmlt123/agent-vocal-ia-RAG-Agentique.git\"\n",
    "branch = \"pipecat-local-colab\"\n",
    "repo_name = \"agent-vocal-ia-RAG-Agentique\"\n",
    "\n",
    "if not os.path.exists(repo_name):\n",
    "    !git clone -b {branch} {repo_url}\n",
    "    print(f\"âœ… Repository clonÃ© (branche: {branch})\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸  Repository dÃ©jÃ  prÃ©sent\")\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(repo_name)\n",
    "print(f\"\\nğŸ“ RÃ©pertoire de travail : {os.getcwd()}\")\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "print(\"\\nâœ… Setup complet !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Ã‰tape 4 : TÃ©lÃ©chargement des ModÃ¨les Whisper et Piper\n",
    "\n",
    "TÃ©lÃ©chargement des modÃ¨les pour STT et TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ TÃ©lÃ©chargement des modÃ¨les...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Whisper will be downloaded automatically by faster-whisper on first use\n",
    "print(\"âœ… Whisper : sera tÃ©lÃ©chargÃ© automatiquement au premier usage\")\n",
    "\n",
    "# Download Piper voice model\n",
    "print(\"\\nğŸ“¥ TÃ©lÃ©chargement de la voix Piper (fr_FR-siwis-medium)...\")\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "os.makedirs(\"models/voices\", exist_ok=True)\n",
    "\n",
    "voice_url = \"https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx\"\n",
    "config_url = \"https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx.json\"\n",
    "\n",
    "voice_path = \"models/voices/fr_FR-siwis-medium.onnx\"\n",
    "config_path = \"models/voices/fr_FR-siwis-medium.onnx.json\"\n",
    "\n",
    "if not os.path.exists(voice_path):\n",
    "    print(\"â³ TÃ©lÃ©chargement du modÃ¨le vocal...\")\n",
    "    urllib.request.urlretrieve(voice_url, voice_path)\n",
    "    print(\"âœ… ModÃ¨le vocal tÃ©lÃ©chargÃ©\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  ModÃ¨le vocal dÃ©jÃ  prÃ©sent\")\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    print(\"â³ TÃ©lÃ©chargement de la config...\")\n",
    "    urllib.request.urlretrieve(config_url, config_path)\n",
    "    print(\"âœ… Config tÃ©lÃ©chargÃ©e\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  Config dÃ©jÃ  prÃ©sente\")\n",
    "\n",
    "print(\"\\nâœ… Tous les modÃ¨les sont prÃªts !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ Ã‰tape 5 : Construction des Index RAG\n",
    "\n",
    "CrÃ©ation des index vectoriels pour chaque domaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ—„ï¸  Construction des index RAG...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Utiliser le script standalone (pas de dÃ©pendance src.config/utils)\n",
    "!python scripts/build_indexes_colab.py\n",
    "\n",
    "print(\"\\nâœ… Index RAG construits !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Ã‰tape 6 : Initialisation du Pipeline Pipecat\n",
    "\n",
    "CrÃ©ation et initialisation du pipeline vocal complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ Initialisation du pipeline Pipecat...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Import pipeline\n",
    "from src.pipeline.voice_pipeline import create_voice_pipeline\n",
    "\n",
    "# Create pipeline\n",
    "print(\"\\nâ³ CrÃ©ation du pipeline (cela peut prendre 30-60 secondes)...\\n\")\n",
    "\n",
    "pipeline = await create_voice_pipeline(\n",
    "    whisper_model=\"base\",      # tiny/base/small (base recommandÃ© pour Colab T4)\n",
    "    ollama_model=\"qwen2:1.5b\",  # ModÃ¨le optimisÃ© pour Colab\n",
    "    device=\"cuda\",              # GPU acceleration\n",
    "    rag_data_path=\"data\"        # Chemin vers les donnÃ©es RAG\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Pipeline Pipecat initialisÃ© avec succÃ¨s !\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nğŸ“Š Configuration :\")\n",
    "print(f\"  â€¢ STT      : Whisper base (faster-whisper + CUDA)\")\n",
    "print(f\"  â€¢ LLM      : Ollama Qwen2 1.5B\")\n",
    "print(f\"  â€¢ TTS      : Piper fr_FR-siwis-medium\")\n",
    "print(f\"  â€¢ RAG      : 3 domaines (maths, physique, anglais)\")\n",
    "print(f\"  â€¢ Framework: Pipecat\")\n",
    "print(f\"  â€¢ Device   : CUDA (GPU)\")\n",
    "print(\"\\nğŸš€ PrÃªt pour le traitement !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Ã‰tape 7 : Test Rapide du Pipeline\n",
    "\n",
    "Test avec une question textuelle pour valider le fonctionnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª Test du pipeline avec une question exemple...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_question = \"Comment rÃ©soudre une Ã©quation du second degrÃ© ?\"\n",
    "print(f\"\\nâ“ Question : {test_question}\")\n",
    "print(\"\\nâ³ Traitement en cours...\\n\")\n",
    "\n",
    "# Process the question\n",
    "result = await pipeline.process_text(test_question)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š RÃ‰SULTATS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“ Transcription : {result['transcription']}\")\n",
    "print(f\"\\nğŸ“š Domaine dÃ©tectÃ© : {result['subject']}\")\n",
    "print(f\"\\nğŸ’¡ RÃ©ponse :\\n{result['response']}\")\n",
    "print(f\"\\nğŸ”Š Audio gÃ©nÃ©rÃ© : {len(result['audio_output'])} bytes Ã  {result['sample_rate']} Hz\")\n",
    "print(\"\\nâœ… Test rÃ©ussi ! Le pipeline fonctionne correctement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ Ã‰tape 8 : Lancement de l'Interface Gradio\n",
    "\n",
    "Interface graphique complÃ¨te pour interagir avec l'agent vocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¨ Lancement de l'interface Gradio...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from src.ui.ui_gradio_pipecat import create_gradio_app\n",
    "\n",
    "# Create Gradio app with the pipeline\n",
    "app = create_gradio_app(pipeline)\n",
    "\n",
    "# Build and launch interface\n",
    "app.build_interface()\n",
    "\n",
    "print(\"\\nğŸš€ Lancement de l'interface...\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Interface Gradio en cours de dÃ©marrage...\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nğŸ“± Un lien public va apparaÃ®tre ci-dessous dans quelques secondes.\")\n",
    "print(\"   Cliquez sur ce lien pour accÃ©der Ã  l'interface !\")\n",
    "print(\"\\nğŸ’¡ Vous pouvez :\")\n",
    "print(\"   â€¢ Utiliser le microphone pour poser des questions vocalement\")\n",
    "print(\"   â€¢ Taper vos questions directement\")\n",
    "print(\"   â€¢ Ã‰couter les rÃ©ponses synthÃ©tisÃ©es\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "app.launch(\n",
    "    share=True,          # Create public link for Colab\n",
    "    server_port=7860\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Informations ComplÃ©mentaires\n",
    "\n",
    "### Architecture Pipecat\n",
    "\n",
    "Le pipeline est composÃ© des processeurs suivants dans l'ordre :\n",
    "\n",
    "1. **LocalSTTService** : Whisper (faster-whisper) pour la transcription\n",
    "2. **TranscriptionCollector** : Collecte la transcription\n",
    "3. **RAGService** : DÃ©tection du domaine + retrieval de documents\n",
    "4. **LocalLLMService** : Ollama pour la gÃ©nÃ©ration de rÃ©ponse\n",
    "5. **ResponseCollector** : Collecte la rÃ©ponse\n",
    "6. **LocalTTSService** : Piper pour la synthÃ¨se vocale\n",
    "7. **AudioBufferProcessor** : Collecte l'audio de sortie\n",
    "\n",
    "### Performance Attendue (Colab T4)\n",
    "\n",
    "| Composant | Latence |\n",
    "|-----------|--------|\n",
    "| STT (Whisper base) | ~200ms |\n",
    "| RAG (retrieval + routing) | ~100ms |\n",
    "| LLM (Qwen2 1.5B) | ~800ms |\n",
    "| TTS (Piper) | ~300ms |\n",
    "| **Total** | **~1.4s** |\n",
    "\n",
    "### Avantages de Pipecat\n",
    "\n",
    "- âœ… **Streaming** : Traitement asynchrone des frames\n",
    "- âœ… **Modulaire** : Facile d'ajouter/remplacer des composants\n",
    "- âœ… **Performant** : Pipeline optimisÃ© pour la latence\n",
    "- âœ… **Flexible** : Support audio + texte\n",
    "\n",
    "### Utilisation\n",
    "\n",
    "1. **Onglet \"EntrÃ©e Vocale\"** : Cliquez sur le micro, posez votre question\n",
    "2. **Onglet \"EntrÃ©e Texte\"** : Tapez votre question ou utilisez les exemples\n",
    "3. Attendez la rÃ©ponse (1-2 secondes)\n",
    "4. Ã‰coutez la rÃ©ponse vocale gÃ©nÃ©rÃ©e\n",
    "\n",
    "### Questions Exemples\n",
    "\n",
    "**MathÃ©matiques** ğŸ”¢\n",
    "- Comment rÃ©soudre une Ã©quation du second degrÃ© ?\n",
    "- Explique-moi le thÃ©orÃ¨me de Pythagore\n",
    "\n",
    "**Physique** âš›ï¸\n",
    "- Qu'est-ce que la force de gravitation ?\n",
    "- Quelle est la troisiÃ¨me loi de Newton ?\n",
    "\n",
    "**Anglais** ğŸ‡¬ğŸ‡§\n",
    "- Comment conjuguer le verbe 'to be' au prÃ©sent ?\n",
    "- Comment utiliser le present perfect ?\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ Projet rÃ©alisÃ© par** : Romain Mallet  \n",
    "**ğŸ“… Date** : Novembre 2024  \n",
    "**ğŸ”— Repository** : [GitHub](https://github.com/Romainmlt123/agent-vocal-ia-RAG-Agentique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}