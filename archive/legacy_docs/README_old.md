# Agent Vocal Prof ğŸ“ğŸ¤

> A fully local, streaming voice tutoring agent with RAG-powered multi-subject support and agentic routing to specialized small language models.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## ğŸ¯ Project Overview

**Agent Vocal Prof** is a professional voice tutoring system that runs 100% locally, with no external APIs or API keys required. It combines:

- **ğŸ™ï¸ Streaming Audio I/O**: Real-time speech recognition and text-to-speech
- **ğŸ“š Multi-Subject RAG**: Retrieval-augmented generation for Math, Physics, and English
- **ğŸ¤– Agentic Routing**: Intelligent selection of specialized small language models based on subject matter
- **ğŸ“ Pedagogical Design**: 3-level hint ladder that guides students without giving away answers
- **ğŸ–±ï¸ Push-to-Talk UI**: Simple Gradio interface for interactive tutoring sessions

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Gradio UI (Push-to-Talk)                  â”‚
â”‚  [Start/Stop] | Live Transcript | Hint Ladder | RAG Sources      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Orchestrator                                â”‚
â”‚  Session State Management | Event Pipeline | Error Handling      â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚        â”‚        â”‚        â”‚        â”‚
   â–¼        â–¼        â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ ASR â”‚ â”‚ RAG â”‚ â”‚Routeâ”‚ â”‚ LLM â”‚ â”‚ TTS â”‚
â”‚     â”‚ â”‚     â”‚ â”‚     â”‚ â”‚     â”‚ â”‚     â”‚
â”‚Whispâ”‚ â”‚FAISSâ”‚ â”‚TF-  â”‚ â”‚llamaâ”‚ â”‚Piperâ”‚
â”‚er+  â”‚ â”‚sent.â”‚ â”‚IDF  â”‚ â”‚.cpp â”‚ â”‚     â”‚
â”‚VAD  â”‚ â”‚transâ”‚ â”‚key- â”‚ â”‚GGUF â”‚ â”‚FR/ENâ”‚
â”‚     â”‚ â”‚form â”‚ â”‚wordsâ”‚ â”‚     â”‚ â”‚     â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
```

### Pipeline Flow

1. **Audio Input** â†’ VAD detects speech â†’ Faster-Whisper transcribes
2. **Transcript** â†’ Router detects subject (math/physics/english)
3. **RAG Retrieval** â†’ Fetch relevant context from subject-specific FAISS index
4. **LLM Generation** â†’ Specialized model generates 3-level hint ladder
5. **Audio Output** â†’ Piper-TTS synthesizes response in FR/EN
6. **UI Update** â†’ Display transcript, hints, sources, and stream audio

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- (Optional) CUDA-capable GPU for faster inference

### Installation - Local (WSL/VSCode)

```bash
# Clone the repository
git clone https://github.com/your-org/intelligence_lab_agent_vocal.git
cd intelligence_lab_agent_vocal

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download models (see models/README.md)
# - LLM: Download Phi-3 or Qwen GGUF models
# - TTS: Download Piper voices for FR/EN

# Build RAG indexes
bash scripts/build_indexes.sh

# Run Gradio UI
bash scripts/run_gradio.sh
```

### Installation - Google Colab

Open and run: [`notebooks/00_setup_colab.ipynb`](notebooks/00_setup_colab.ipynb)

This notebook will:
- Install all dependencies
- Check GPU availability
- Download necessary models
- Run smoke tests
- Launch the Gradio interface

## ğŸ“ Project Structure

```
agent-vocal-prof/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ requirements.txt             # Python dependencies (pinned versions)
â”œâ”€â”€ CHANGELOG.md                 # Version history
â”œâ”€â”€ CONTRIBUTING.md              # Contribution guidelines
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_setup_colab.ipynb    # Colab setup + GPU checks + smoke tests
â”‚   â””â”€â”€ 10_demo_pipeline.ipynb  # End-to-end demo: ASRâ†’RAGâ†’LLMâ†’TTS
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Load config.yaml, environment variables
â”‚   â”œâ”€â”€ asr.py                  # VAD + ASR streaming (Silero + Whisper)
â”‚   â”œâ”€â”€ rag_build.py            # Document ingestion â†’ embeddings â†’ FAISS
â”‚   â”œâ”€â”€ rag.py                  # Retrieve relevant passages per subject
â”‚   â”œâ”€â”€ router.py               # Subject detection + model routing
â”‚   â”œâ”€â”€ llm.py                  # llama-cpp wrapper with streaming
â”‚   â”œâ”€â”€ tts.py                  # Piper-TTS for FR/EN speech synthesis
â”‚   â”œâ”€â”€ orchestrator.py         # Full pipeline orchestration + state
â”‚   â”œâ”€â”€ ui_gradio.py            # Gradio push-to-talk interface
â”‚   â””â”€â”€ utils.py                # Logging, file I/O, helpers
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml             # Models, paths, chunk sizes, n_ctx, etc.
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ maths/                  # Math PDF/TXT documents
â”‚   â”œâ”€â”€ physique/               # Physics PDF/TXT documents
â”‚   â””â”€â”€ anglais/                # English PDF/TXT documents
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ llm/                    # GGUF model files (not committed)
â”‚   â”œâ”€â”€ voices/                 # Piper voice models (not committed)
â”‚   â””â”€â”€ README.md               # Download instructions
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_rag.py
â”‚   â”œâ”€â”€ test_llm.py
â”‚   â”œâ”€â”€ test_asr.py
â”‚   â”œâ”€â”€ test_tts.py
â”‚   â””â”€â”€ test_orch.py
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ build_indexes.sh        # Build FAISS indexes for all subjects
    â””â”€â”€ run_gradio.sh           # Launch Gradio UI
```

## ğŸ“ Pedagogical Features

### 3-Level Hint Ladder

The agent never provides direct solutions. Instead, it offers progressively detailed hints:

1. **Level 1 - Conceptual Hint**: High-level guidance pointing to the relevant concept
2. **Level 2 - Strategic Hint**: Specific approach or method to use
3. **Level 3 - Detailed Hint**: Step-by-step breakdown (but still requires student to execute)

### RAG Source Traceability

All responses display:
- Source document title
- Page number
- Relevance score
- Excerpt snippet

This ensures transparency and allows students to verify information.

## ğŸ”§ Configuration

Edit `config/config.yaml` to customize:

```yaml
# ASR settings
asr:
  model: "base"  # tiny, base, small, medium, large
  language: "fr"
  vad_threshold: 0.5

# RAG settings
rag:
  chunk_size: 512
  chunk_overlap: 50
  top_k: 4
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"

# LLM settings
llm:
  models:
    maths: "models/llm/phi-3-mini-4k-instruct-q4.gguf"
    physique: "models/llm/qwen2-1.5b-instruct-q4.gguf"
    anglais: "models/llm/phi-3-mini-4k-instruct-q4.gguf"
  n_ctx: 4096
  temperature: 0.7
  max_tokens: 512

# TTS settings
tts:
  voice_fr: "models/voices/fr_FR-siwis-medium.onnx"
  voice_en: "models/voices/en_US-lessac-medium.onnx"
  speed: 1.0
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test module
pytest tests/test_rag.py -v

# Lint code
flake8 src/ tests/
black src/ tests/ --check
```

## ğŸ“Š Performance Tips

### For Google Colab

- Use GPU runtime for faster inference
- Enable high-RAM if processing large documents
- Cache models in Google Drive to avoid re-downloading

### For Local Development

- Use quantized GGUF models (Q4_K_M recommended)
- Limit `n_ctx` to 4096 for faster generation
- Use `faster-whisper` base model for balanced speed/accuracy
- Enable CPU optimizations: `OMP_NUM_THREADS=4`

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

Key areas for contribution:
- Additional subject domains
- Improved routing algorithms
- Better prompt engineering for hint generation
- UI/UX enhancements
- Documentation and examples

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

Built with:
- [faster-whisper](https://github.com/guillaumekln/faster-whisper) - Fast ASR
- [sentence-transformers](https://www.sbert.net/) - Embeddings
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) - Local LLM inference
- [piper-tts](https://github.com/rhasspy/piper) - Neural TTS
- [Gradio](https://www.gradio.app/) - UI framework

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue.

---

**Note**: This is a fully local system with no external API dependencies. All processing happens on your machine or Colab instance.
