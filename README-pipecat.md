# ğŸ™ï¸ Agent Vocal IA - Architecture Pipecat

> **Branche**: `pipecat-local-colab`  
> **Objectif**: Agent vocal IA en temps rÃ©el avec RAG agentique, 100% local, optimisÃ© pour Google Colab

---

## ğŸ¯ Vue d'ensemble

Cette version du projet utilise le framework **Pipecat** pour crÃ©er un agent vocal IA conversationnel en temps rÃ©el, capable de :

- ğŸ¤ **Reconnaissance vocale** locale (Whisper)
- ğŸ¤– **GÃ©nÃ©ration de rÃ©ponses** avec LLM local (Ollama + Llama 3.2)
- ğŸ”Š **SynthÃ¨se vocale** locale (Piper TTS)
- ğŸ“š **RAG Agentique** multi-matiÃ¨res (Maths, Physique, Anglais)
- âš¡ **Latence <2s** (objectif streaming temps rÃ©el)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Utilisateur (Microphone)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Pipecat Pipeline                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STT   â”‚â†’ â”‚  RAG   â”‚â†’ â”‚  LLM   â”‚â†’ â”‚  TTS   â”‚â†’ â”‚ Audio  â”‚  â”‚
â”‚  â”‚Whisper â”‚  â”‚ChromaDBâ”‚  â”‚ Ollama â”‚  â”‚ Piper  â”‚  â”‚ Out    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants ClÃ©s

#### 1. **Services Locaux** (`src/services/`)

- `local_stt.py` : Whisper (faster-whisper) pour transcription
- `local_llm.py` : Ollama client pour LLM local
- `local_tts.py` : Piper TTS pour synthÃ¨se vocale franÃ§aise
- `rag_service.py` : RAG avec ChromaDB + routing multi-matiÃ¨res

#### 2. **Pipeline Pipecat**

Utilise les `FrameProcessor` de Pipecat pour traiter les flux audio/texte :

```python
pipeline = Pipeline([
    LocalSTTService(model="base"),      # Audio â†’ Text
    RAGService(subjects=["maths"]),     # Enrichissement contextuel
    LocalLLMService(model="llama3.2"),  # GÃ©nÃ©ration rÃ©ponse
    LocalTTSService(voice="fr_FR"),     # Text â†’ Audio
])
```

#### 3. **RAG Agentique**

- **Routing intelligent** : DÃ©tecte automatiquement la matiÃ¨re (maths/physique/anglais)
- **Vectorstores sÃ©parÃ©s** : Un index ChromaDB par matiÃ¨re
- **Embeddings locaux** : sentence-transformers (all-MiniLM-L6-v2)

---

## ğŸ“‚ Structure du Projet

```
agent-vocal-ia-RAG-Agentique/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_setup_colab_pipecat.ipynb     # Setup Colab complet
â”‚   â”œâ”€â”€ 02_test_components.ipynb         # Tests unitaires
â”‚   â”œâ”€â”€ 03_full_agent_demo.ipynb         # Demo complÃ¨te
â”‚   â””â”€â”€ 04_advanced_rag.ipynb            # RAG agentique avancÃ©
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ local_stt.py                 # Whisper STT
â”‚   â”‚   â”œâ”€â”€ local_llm.py                 # Ollama LLM
â”‚   â”‚   â”œâ”€â”€ local_tts.py                 # Piper TTS
â”‚   â”‚   â””â”€â”€ rag_service.py               # RAG + Routing
â”‚   â””â”€â”€ agents/
â”‚       â””â”€â”€ voice_agent.py                # Agent principal
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ maths/                            # Documents maths
â”‚   â”œâ”€â”€ physique/                         # Documents physique
â”‚   â””â”€â”€ anglais/                          # Documents anglais
â”œâ”€â”€ requirements-colab.txt                # DÃ©pendances Colab
â””â”€â”€ README-pipecat.md                     # Ce fichier
```

---

## ğŸš€ Utilisation sur Google Colab

### Ã‰tape 1 : Ouvrir le Notebook

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Romainmlt123/agent-vocal-ia-RAG-Agentique/blob/pipecat-local-colab/notebooks/01_setup_colab_pipecat.ipynb)

### Ã‰tape 2 : Activer le GPU

1. Menu : **Runtime** > **Change runtime type**
2. Hardware accelerator : **GPU** (T4 ou A100)
3. Cliquez sur **Save**

### Ã‰tape 3 : ExÃ©cuter le Setup

ExÃ©cutez toutes les cellules du notebook `01_setup_colab_pipecat.ipynb` :

- Installation des dÃ©pendances systÃ¨me
- Installation de Pipecat et packages Python
- Installation d'Ollama + tÃ©lÃ©chargement du modÃ¨le
- TÃ©lÃ©chargement de Whisper
- Configuration de Piper TTS
- Construction des index RAG

**â±ï¸ Temps estimÃ©** : 10-15 minutes

### Ã‰tape 4 : Tester l'Agent

Une fois le setup terminÃ©, passez au notebook `03_full_agent_demo.ipynb` pour interagir avec l'agent vocal.

---

## ğŸ’» Utilisation Locale (WSL/Linux)

Si vous voulez dÃ©velopper localement avec un GPU :

```bash
# Cloner le projet
git clone -b pipecat-local-colab https://github.com/Romainmlt123/agent-vocal-ia-RAG-Agentique.git
cd agent-vocal-ia-RAG-Agentique

# CrÃ©er l'environnement virtuel
python3 -m venv venv
source venv/bin/activate

# Installer les dÃ©pendances
pip install -r requirements-colab.txt

# Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2:1b

# Lancer Ollama
ollama serve &

# Construire les index RAG
python scripts/build_rag_indexes.py

# Tester l'agent
python src/agents/voice_agent.py
```

---

## ğŸ“Š Performance

### Latences MesurÃ©es (Google Colab T4)

| Composant | Latence | ModÃ¨le |
|-----------|---------|--------|
| **STT** (Whisper) | ~200ms | base (74M params) |
| **RAG** (Retrieval) | ~100ms | all-MiniLM-L6-v2 |
| **LLM** (Ollama) | ~800ms | llama3.2:1b |
| **TTS** (Piper) | ~300ms | fr_FR-siwis-medium |
| **Total** | **~1.4s** | Pipeline complet |

### Optimisations

- âœ… Utilisation de `faster-whisper` (vs whisper OpenAI)
- âœ… ModÃ¨le LLM lÃ©ger (1B params au lieu de 7B+)
- âœ… Piper TTS (plus rapide que Coqui/Bark)
- âœ… Embeddings cachÃ©s en RAM
- âœ… Streaming token-by-token pour LLM
- âœ… GPU acceleration pour tous les modÃ¨les

---

## ğŸ”§ Configuration

### ModÃ¨les Disponibles

#### STT (Whisper)
- `tiny` : 39M params, ~100ms, 60% prÃ©cision
- `base` : 74M params, ~200ms, 75% prÃ©cision âœ… **RecommandÃ©**
- `small` : 244M params, ~500ms, 85% prÃ©cision
- `medium` : 769M params, ~1.5s, 90% prÃ©cision

#### LLM (Ollama)
- `llama3.2:1b` : 1B params, ~800ms âœ… **RecommandÃ© Colab**
- `llama3.2:3b` : 3B params, ~2s
- `mistral:7b` : 7B params, ~5s (nÃ©cessite A100)

#### TTS (Piper)
- `fr_FR-siwis-medium` : FranÃ§ais naturel âœ…
- `en_US-lessac-medium` : Anglais amÃ©ricain

### Personnalisation

Modifier les paramÃ¨tres dans le notebook ou via le code :

```python
from src.services.local_stt import LocalSTTService
from src.services.local_llm import LocalLLMService
from src.services.local_tts import LocalTTSService

# Configuration personnalisÃ©e
stt = LocalSTTService(
    model_size="small",  # Changer le modÃ¨le
    language="fr",
    device="cuda"
)

llm = LocalLLMService(
    model="mistral:7b",  # Changer le modÃ¨le
    temperature=0.8,
    max_tokens=1024
)

tts = LocalTTSService(
    voice_model="en_US-lessac-medium",  # Voix anglaise
    speed=1.2
)
```

---

## ğŸ› DÃ©pannage

### ProblÃ¨me : Pas de GPU dÃ©tectÃ©

**Solution** :
1. Runtime > Change runtime type > GPU
2. RedÃ©marrer le runtime
3. VÃ©rifier avec `!nvidia-smi`

### ProblÃ¨me : Ollama ne rÃ©pond pas

**Solution** :
```python
# RedÃ©marrer Ollama
!pkill ollama
!ollama serve &
import time; time.sleep(5)
```

### ProblÃ¨me : Out of Memory (OOM)

**Solution** :
- Utiliser `llama3.2:1b` au lieu de modÃ¨les plus gros
- RÃ©duire `max_tokens` du LLM
- Utiliser Whisper `tiny` ou `base`

### ProblÃ¨me : Latence trop Ã©levÃ©e

**Optimisations** :
1. RÃ©duire la taille des modÃ¨les
2. Activer le streaming LLM
3. DÃ©sactiver le RAG pour les tests
4. Utiliser le batch processing

---

## ğŸ“š Documentation ComplÃ¨te

- [Pipecat Documentation](https://docs.pipecat.ai/)
- [Ollama Models](https://ollama.com/library)
- [Whisper GitHub](https://github.com/openai/whisper)
- [Piper TTS](https://github.com/rhasspy/piper)
- [LangChain RAG Guide](https://python.langchain.com/docs/use_cases/question_answering/)

---

## ğŸ¤ Contribution

Cette branche est expÃ©rimentale. Les PR sont bienvenues pour :

- AmÃ©liorer la latence
- Ajouter de nouveaux modÃ¨les
- Optimiser le RAG
- AmÃ©liorer la documentation

---

## ğŸ“ Licence

MIT License - Voir [LICENSE](../LICENSE)

---

## ğŸ“ Auteur

DÃ©veloppÃ© dans le cadre du projet Agent Vocal IA - RAG Agentique

**Contact** : [GitHub](https://github.com/Romainmlt123)

---

**ğŸš€ PrÃªt Ã  commencer ? Ouvrez le notebook sur Colab !**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Romainmlt123/agent-vocal-ia-RAG-Agentique/blob/pipecat-local-colab/notebooks/01_setup_colab_pipecat.ipynb)
