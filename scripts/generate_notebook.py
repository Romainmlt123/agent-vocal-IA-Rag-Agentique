"""
Script pour g√©n√©rer le notebook de setup Colab complet.
Ce script cr√©e un notebook Jupyter avec toutes les cellules n√©cessaires.
"""

import json
from pathlib import Path

# D√©finir les cellules du notebook
cells = [
    # Cell 1: Titre
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# üöÄ Setup Google Colab - Agent Vocal IA avec Pipecat\n",
            "\n",
            "Ce notebook configure un environnement Google Colab complet pour d√©velopper un agent vocal IA en temps r√©el avec :\n",
            "- **Pipecat** : Framework pour agents vocaux\n",
            "- **Whisper** : STT (Speech-to-Text) local\n",
            "- **Ollama** : LLM local\n",
            "- **Piper TTS** : Synth√®se vocale locale\n",
            "- **RAG** : Recherche documentaire avec ChromaDB\n",
            "\n",
            "## ‚ö†Ô∏è Pr√©requis\n",
            "- Compte Google Colab\n",
            "- **GPU T4/A100** activ√© (Runtime > Change runtime type > GPU)\n",
            "\n",
            "---"
        ]
    },
    
    # Cell 2: V√©rification GPU
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üìä √âtape 1 : V√©rification du GPU\n",
            "\n",
            "V√©rifions que vous avez bien acc√®s √† un GPU pour acc√©l√©rer l'inf√©rence des mod√®les."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# V√©rifier la disponibilit√© du GPU\n",
            "!nvidia-smi\n",
            "\n",
            "import torch\n",
            "print(f\"\\n‚úÖ PyTorch version: {torch.__version__}\")\n",
            "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
            "if torch.cuda.is_available():\n",
            "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
            "    print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
            "else:\n",
            "    print(\"‚ö†Ô∏è  Pas de GPU d√©tect√© ! Allez dans Runtime > Change runtime type > GPU\")"
        ]
    },
    
    # Cell 3: Cloner le projet
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üì¶ √âtape 2 : Cloner le Projet\n",
            "\n",
            "Clonons le repository GitHub contenant notre agent vocal."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Cloner le repository\n",
            "!git clone -b pipecat-local-colab https://github.com/Romainmlt123/agent-vocal-ia-RAG-Agentique.git\n",
            "%cd agent-vocal-ia-RAG-Agentique\n",
            "\n",
            "# Afficher la structure\n",
            "!ls -la"
        ]
    },
    
    # Cell 4: Installation d√©pendances syst√®me
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üîß √âtape 3 : Installation des D√©pendances Syst√®me\n",
            "\n",
            "Installons les biblioth√®ques syst√®me n√©cessaires pour l'audio."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Mise √† jour du syst√®me et installation des d√©pendances audio\n",
            "!apt-get update -qq\n",
            "!apt-get install -y -qq portaudio19-dev python3-pyaudio ffmpeg espeak-ng libsndfile1\n",
            "\n",
            "print(\"‚úÖ D√©pendances syst√®me install√©es\")"
        ]
    },
    
    # Cell 5: Installation Python packages
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üìö √âtape 4 : Installation des Packages Python\n",
            "\n",
            "Installons tous les packages Python n√©cessaires (cela peut prendre 5-10 minutes)."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Installation des packages Python depuis requirements-colab.txt\n",
            "!pip install -q -r requirements-colab.txt\n",
            "\n",
            "# V√©rifier les installations critiques\n",
            "import sys\n",
            "packages_to_check = [\n",
            "    'pipecat',\n",
            "    'torch',\n",
            "    'whisper',\n",
            "    'faster_whisper',\n",
            "    'langchain',\n",
            "    'chromadb'\n",
            "]\n",
            "\n",
            "print(\"\\nüì¶ V√©rification des packages install√©s:\\n\")\n",
            "for pkg in packages_to_check:\n",
            "    try:\n",
            "        __import__(pkg)\n",
            "        print(f\"‚úÖ {pkg}\")\n",
            "    except ImportError:\n",
            "        print(f\"‚ùå {pkg} - √âCHEC\")\n",
            "\n",
            "print(\"\\n‚úÖ Installation des packages Python termin√©e\")"
        ]
    },
    
    # Cell 6: Installation Ollama
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## ü§ñ √âtape 5 : Installation d'Ollama\n",
            "\n",
            "Ollama permet d'ex√©cuter des LLMs localement. Nous allons l'installer et t√©l√©charger un mod√®le l√©ger."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Installer Ollama\n",
            "!curl -fsSL https://ollama.com/install.sh | sh\n",
            "\n",
            "print(\"‚úÖ Ollama install√©\")"
        ]
    },
    
    # Cell 7: D√©marrer Ollama
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# D√©marrer le serveur Ollama en arri√®re-plan\n",
            "import subprocess\n",
            "import time\n",
            "\n",
            "print(\"üöÄ D√©marrage du serveur Ollama...\")\n",
            "ollama_process = subprocess.Popen(\n",
            "    ['ollama', 'serve'],\n",
            "    stdout=subprocess.PIPE,\n",
            "    stderr=subprocess.PIPE\n",
            ")\n",
            "\n",
            "# Attendre le d√©marrage\n",
            "time.sleep(5)\n",
            "print(\"‚úÖ Serveur Ollama d√©marr√©\")"
        ]
    },
    
    # Cell 8: T√©l√©charger mod√®le Ollama
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# T√©l√©charger un mod√®le l√©ger (llama3.2:1b adapt√© √† Colab T4)\n",
            "print(\"üì• T√©l√©chargement du mod√®le Llama 3.2 (1B)...\")\n",
            "print(\"‚è±Ô∏è  Cela peut prendre 2-5 minutes selon votre connexion\\n\")\n",
            "\n",
            "!ollama pull llama3.2:1b\n",
            "\n",
            "print(\"\\n‚úÖ Mod√®le t√©l√©charg√© et pr√™t √† l'emploi\")"
        ]
    },
    
    # Cell 9: T√©l√©charger Whisper
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üé§ √âtape 6 : Pr√©paration de Whisper (STT)\n",
            "\n",
            "T√©l√©chargeons le mod√®le Whisper pour la reconnaissance vocale."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# T√©l√©charger le mod√®le Whisper\n",
            "from faster_whisper import WhisperModel\n",
            "\n",
            "print(\"üì• T√©l√©chargement du mod√®le Whisper (base)...\")\n",
            "model = WhisperModel(\"base\", device=\"cuda\", compute_type=\"float16\")\n",
            "print(\"‚úÖ Mod√®le Whisper t√©l√©charg√©\")\n",
            "\n",
            "# Test rapide\n",
            "print(\"\\nüß™ Test de Whisper...\")\n",
            "# Le test complet sera fait dans le prochain notebook"
        ]
    },
    
    # Cell 10: Configuration Piper TTS
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üîä √âtape 7 : Configuration de Piper TTS\n",
            "\n",
            "Piper TTS pour la synth√®se vocale locale en fran√ßais."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Cr√©er le dossier pour les voix Piper\n",
            "!mkdir -p /root/.local/share/piper/voices\n",
            "\n",
            "# T√©l√©charger une voix fran√ßaise\n",
            "print(\"üì• T√©l√©chargement de la voix fran√ßaise Piper...\")\n",
            "!wget -q -O /root/.local/share/piper/voices/fr_FR-siwis-medium.onnx \\\n",
            "    https://github.com/rhasspy/piper/releases/download/v1.2.0/fr_FR-siwis-medium.onnx\n",
            "\n",
            "print(\"‚úÖ Voix Piper TTS t√©l√©charg√©e\")"
        ]
    },
    
    # Cell 11: Build RAG indexes
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üìö √âtape 8 : Construction des Index RAG\n",
            "\n",
            "Construisons les index vectoriels pour la recherche documentaire."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Importer le service RAG\n",
            "import sys\n",
            "sys.path.append('/content/agent-vocal-ia-RAG-Agentique/src')\n",
            "\n",
            "from services.rag_service import AgenticRAGService\n",
            "import asyncio\n",
            "import nest_asyncio\n",
            "nest_asyncio.apply()\n",
            "\n",
            "print(\"üîß Initialisation du service RAG...\")\n",
            "\n",
            "# Cr√©er le service RAG\n",
            "rag_service = AgenticRAGService(\n",
            "    base_path=\"./data\",\n",
            "    subjects=[\"maths\", \"physique\", \"anglais\"]\n",
            ")\n",
            "\n",
            "# Charger les documents\n",
            "async def load_documents():\n",
            "    await rag_service._initialize()\n",
            "    \n",
            "    # Charger chaque mati√®re\n",
            "    for subject in [\"maths\", \"physique\", \"anglais\"]:\n",
            "        subject_path = f\"./data/{subject}\"\n",
            "        print(f\"\\nüìñ Chargement des documents {subject}...\")\n",
            "        await rag_service.load_documents_from_directory(\n",
            "            subject_path,\n",
            "            subject=subject\n",
            "        )\n",
            "    \n",
            "    print(\"\\n‚úÖ Tous les documents sont index√©s !\")\n",
            "\n",
            "# Ex√©cuter\n",
            "await load_documents()"
        ]
    },
    
    # Cell 12: R√©sum√©
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## ‚úÖ Configuration Termin√©e !\n",
            "\n",
            "### üìã R√©capitulatif\n",
            "\n",
            "Votre environnement Colab est maintenant configur√© avec :\n",
            "\n",
            "- ‚úÖ GPU d√©tect√© et PyTorch configur√©\n",
            "- ‚úÖ Pipecat framework install√©\n",
            "- ‚úÖ Whisper (base) pour la reconnaissance vocale\n",
            "- ‚úÖ Ollama + Llama 3.2 (1B) pour le LLM\n",
            "- ‚úÖ Piper TTS pour la synth√®se vocale fran√ßaise\n",
            "- ‚úÖ RAG avec ChromaDB et documents index√©s\n",
            "\n",
            "### üöÄ Prochaines √âtapes\n",
            "\n",
            "1. **Notebook 02** : Test des composants individuels\n",
            "2. **Notebook 03** : Demo compl√®te de l'agent vocal\n",
            "3. **Notebook 04** : RAG agentique avanc√©\n",
            "\n",
            "---\n",
            "\n",
            "üí° **Astuce** : Sauvegardez votre session Colab pour √©viter de recommencer l'installation !"
        ]
    },
    
    # Cell 13: Test rapide
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üß™ Test Rapide (Optionnel)\n",
            "\n",
            "Testons rapidement le LLM Ollama."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Test rapide d'Ollama\n",
            "!ollama run llama3.2:1b \"Explique-moi en une phrase ce qu'est un agent vocal IA\""
        ]
    }
]

# Cr√©er le notebook
notebook = {
    "cells": cells,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

# Sauvegarder
output_path = Path("notebooks/01_setup_colab_pipecat.ipynb")
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print(f"‚úÖ Notebook cr√©√© : {output_path}")
print(f"üìä Nombre de cellules : {len(cells)}")
