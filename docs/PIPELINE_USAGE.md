# ğŸš€ Guide d'Utilisation - Pipeline Pipecat

## ğŸ“‹ Vue d'ensemble

Ce guide explique comment utiliser le **pipeline Pipecat** pour l'agent vocal IA, optimisÃ© pour Google Colab.

---

## ğŸ¯ DÃ©marrage Rapide (Google Colab)

### Option 1 : Utiliser le Notebook Complet

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Romainmlt123/agent-vocal-ia-RAG-Agentique/blob/pipecat-local-colab/notebooks/demo_pipecat_colab.ipynb)

1. **Ouvrir le notebook** : Cliquez sur le badge ci-dessus
2. **Activer le GPU** : `Runtime â†’ Change runtime type â†’ GPU (T4)`
3. **ExÃ©cuter toutes les cellules** : `Runtime â†’ Run all`
4. **Attendre** : ~10-12 minutes pour l'installation complÃ¨te
5. **Utiliser l'interface** : Un lien public Gradio apparaÃ®tra

### Option 2 : Installation Manuelle

```bash
# 1. Cloner le repository
git clone -b pipecat-local-colab https://github.com/Romainmlt123/agent-vocal-ia-RAG-Agentique.git
cd agent-vocal-ia-RAG-Agentique

# 2. Installer les dÃ©pendances
pip install -r requirements-colab.txt

# 3. Installer et dÃ©marrer Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama serve &
ollama pull qwen2:1.5b

# 4. Construire les index RAG
python -m src.legacy.rag_build

# 5. Lancer l'interface
python -m src.ui.ui_gradio_pipecat
```

---

## ğŸ—ï¸ Architecture du Pipeline

### Composants Pipecat

```python
from src.pipeline.voice_pipeline import create_voice_pipeline

# CrÃ©er le pipeline
pipeline = await create_voice_pipeline(
    whisper_model="base",      # Taille du modÃ¨le Whisper
    ollama_model="qwen2:1.5b", # ModÃ¨le LLM
    device="cuda",             # GPU acceleration
    rag_data_path="data"       # Chemin donnÃ©es RAG
)
```

### Flux de Traitement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PIPELINE PIPECAT                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  AudioRawFrame                                               â”‚
â”‚      â†“                                                       â”‚
â”‚  LocalSTTService (Whisper)                                   â”‚
â”‚      â†“                                                       â”‚
â”‚  TranscriptionFrame                                          â”‚
â”‚      â†“                                                       â”‚
â”‚  RAGService (Router + Retrieval)                             â”‚
â”‚      â†“                                                       â”‚
â”‚  TextFrame (with context)                                    â”‚
â”‚      â†“                                                       â”‚
â”‚  LocalLLMService (Ollama)                                    â”‚
â”‚      â†“                                                       â”‚
â”‚  TextFrame (response)                                        â”‚
â”‚      â†“                                                       â”‚
â”‚  LocalTTSService (Piper)                                     â”‚
â”‚      â†“                                                       â”‚
â”‚  TTSAudioRawFrame                                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Utilisation du Pipeline

### 1. Traitement Audio

```python
import numpy as np

# Audio input (PCM 16-bit, 16kHz, mono)
audio_bytes = audio_array.tobytes()
sample_rate = 16000

# Process
result = await pipeline.process_audio(audio_bytes, sample_rate)

print(f"Transcription: {result['transcription']}")
print(f"Subject: {result['subject']}")
print(f"Response: {result['response']}")
print(f"Audio output: {len(result['audio_output'])} bytes")
```

### 2. Traitement Texte

```python
# Text input
question = "Comment rÃ©soudre une Ã©quation du second degrÃ© ?"

# Process
result = await pipeline.process_text(question)

print(f"Subject: {result['subject']}")
print(f"Response: {result['response']}")
```

### 3. Avec Interface Gradio

```python
from src.ui.ui_gradio_pipecat import create_gradio_app

# Create app
app = create_gradio_app(pipeline)

# Build and launch
app.build_interface()
app.launch(share=True, server_port=7860)
```

---

## âš™ï¸ Configuration

### ModÃ¨les Whisper Disponibles

| ModÃ¨le | Taille | VRAM | Latence | QualitÃ© |
|--------|--------|------|---------|---------|
| `tiny` | 39M | 1GB | 100ms | Acceptable |
| `base` | 74M | 1GB | 200ms | **RecommandÃ©** |
| `small` | 244M | 2GB | 400ms | Bonne |
| `medium` | 769M | 5GB | 800ms | TrÃ¨s bonne |
| `large` | 1550M | 10GB | 1.5s | Excellente |

**Pour Colab T4 : Utiliser `base` ou `small`**

### ModÃ¨les Ollama RecommandÃ©s

| ModÃ¨le | Taille | VRAM | Latence | Description |
|--------|--------|------|---------|-------------|
| `qwen2:1.5b` | 900MB | 2GB | 800ms | **RecommandÃ© pour Colab** |
| `llama3.2:1b` | 700MB | 2GB | 600ms | TrÃ¨s rapide |
| `llama3.2:3b` | 2GB | 4GB | 1.2s | Plus prÃ©cis |
| `mistral:7b` | 4GB | 8GB | 3s | Meilleure qualitÃ© (nÃ©cessite A100) |

### Variables d'Environnement

```python
# Configuration du pipeline
WHISPER_MODEL = "base"          # tiny/base/small/medium/large
OLLAMA_MODEL = "qwen2:1.5b"     # ModÃ¨le Ollama
DEVICE = "cuda"                 # cuda/cpu
RAG_DATA_PATH = "data"          # Chemin donnÃ©es RAG
GRADIO_PORT = 7860              # Port Gradio
```

---

## ğŸ¨ Interface Gradio

### FonctionnalitÃ©s

#### Onglet "EntrÃ©e Vocale" ğŸ™ï¸
1. Cliquer sur le microphone
2. Parler clairement
3. Cliquer sur "Traiter l'audio"
4. Attendre les rÃ©sultats (1-2s)

#### Onglet "EntrÃ©e Texte" âŒ¨ï¸
1. Taper une question
2. Cliquer sur "Envoyer"
3. Consulter la rÃ©ponse
4. Ã‰couter l'audio gÃ©nÃ©rÃ©

### Questions Exemples

**MathÃ©matiques** ğŸ”¢
```
- Comment rÃ©soudre une Ã©quation du second degrÃ© ?
- Explique-moi le thÃ©orÃ¨me de Pythagore
- C'est quoi une fonction affine ?
```

**Physique** âš›ï¸
```
- Qu'est-ce que la force de gravitation ?
- Quelle est la troisiÃ¨me loi de Newton ?
- Comment calculer l'Ã©nergie cinÃ©tique ?
```

**Anglais** ğŸ‡¬ğŸ‡§
```
- Comment conjuguer le verbe 'to be' au prÃ©sent ?
- Comment utiliser le present perfect ?
- Quelle est la diffÃ©rence entre 'make' et 'do' ?
```

---

## ğŸ“Š Performance

### Latence MesurÃ©e (Colab T4)

| Composant | Temps | % Total |
|-----------|-------|---------|
| STT (Whisper base) | 200ms | 13% |
| RAG (retrieval + routing) | 100ms | 7% |
| LLM (Qwen2 1.5B) | 800ms | 53% |
| TTS (Piper) | 300ms | 20% |
| Overhead Pipeline | 100ms | 7% |
| **TOTAL** | **1.5s** | **100%** |

### Optimisations AppliquÃ©es

âœ… **Faster-Whisper** : 2x plus rapide que Whisper standard  
âœ… **ModÃ¨le LLM compact** : 1.5B paramÃ¨tres au lieu de 7B+  
âœ… **GPU acceleration** : Tous les modÃ¨les sur CUDA  
âœ… **Streaming** : Traitement asynchrone des frames  
âœ… **Piper TTS** : 3x plus rapide que Coqui/Bark  

---

## ğŸ› DÃ©pannage

### ProblÃ¨me : Pas de GPU dÃ©tectÃ©

```bash
# VÃ©rifier le GPU
!nvidia-smi

# Si vide :
# Runtime â†’ Change runtime type â†’ GPU (T4)
```

### ProblÃ¨me : Ollama ne dÃ©marre pas

```bash
# RedÃ©marrer Ollama
!pkill ollama
!ollama serve &
sleep 5
!ollama list
```

### ProblÃ¨me : Out of Memory (OOM)

```python
# Utiliser des modÃ¨les plus petits
pipeline = await create_voice_pipeline(
    whisper_model="tiny",     # Au lieu de "base"
    ollama_model="qwen2:1.5b" # Le plus compact
)
```

### ProblÃ¨me : Latence trop Ã©levÃ©e

1. **VÃ©rifier le GPU** : `!nvidia-smi`
2. **RÃ©duire les modÃ¨les** : Utiliser `tiny` pour Whisper
3. **Limiter les tokens** : RÃ©duire `max_tokens` dans LocalLLMService
4. **DÃ©sactiver le RAG** : Pour tester uniquement le LLM

---

## ğŸ”§ DÃ©veloppement

### Structure des Fichiers

```
src/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ voice_pipeline.py          # Pipeline Pipecat principal
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ local_stt.py               # Whisper STT service
â”‚   â”œâ”€â”€ local_llm.py               # Ollama LLM service
â”‚   â”œâ”€â”€ local_tts.py               # Piper TTS service
â”‚   â””â”€â”€ rag_service.py             # RAG + routing service
â”‚
â””â”€â”€ ui/
    â”œâ”€â”€ ui_gradio_pipecat.py       # Interface Gradio pour pipeline
    â”œâ”€â”€ ui_gradio.py               # Interface Gradio legacy
    â””â”€â”€ ui_hybrid.py               # Interface hybride
```

### Ajouter un Nouveau Service

```python
from pipecat.processors.frame_processor import FrameProcessor
from pipecat.frames.frames import Frame, FrameDirection

class MyCustomService(FrameProcessor):
    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)
        
        # Process the frame
        # ...
        
        await self.push_frame(frame, direction)
```

### Modifier le Pipeline

```python
# Dans voice_pipeline.py
self.pipeline = Pipeline([
    self.stt_service,
    self.transcription_collector,
    self.rag_service,
    self.my_custom_service,    # Ajouter ici
    self.llm_service,
    self.response_collector,
    self.tts_service,
    self.audio_buffer
])
```

---

## ğŸ“– RÃ©fÃ©rences

- **Pipecat Framework** : https://pipecat.ai/
- **Pipecat GitHub** : https://github.com/pipecat-ai/pipecat
- **Whisper** : https://github.com/openai/whisper
- **Faster-Whisper** : https://github.com/SYSTRAN/faster-whisper
- **Ollama** : https://ollama.com/
- **Piper TTS** : https://github.com/rhasspy/piper
- **Gradio** : https://gradio.app/

---

## ğŸ“§ Support

Pour toute question ou problÃ¨me :

- **Issues GitHub** : [CrÃ©er une issue](https://github.com/Romainmlt123/agent-vocal-ia-RAG-Agentique/issues)
- **Documentation** : `docs/ARCHITECTURE.md`
- **Auteur** : Romain Mallet

---

**ğŸ“ Projet AcadÃ©mique** - Intelligence Lab Agent Vocal  
**ğŸ“… Novembre 2024**
