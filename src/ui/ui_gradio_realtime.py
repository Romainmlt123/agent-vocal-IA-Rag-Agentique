"""
Interface Gradio pour Agent Vocal Temps RÃ©el
Permet une conversation continue via interface web
"""

import asyncio
import numpy as np
import gradio as gr
from loguru import logger
import time
from typing import Optional, Tuple

from src.realtime_voice_agent import RealtimeVoiceAgent, create_realtime_voice_agent


class GradioRealtimeInterface:
    """
    Interface Gradio pour conversation vocale en temps rÃ©el
    """
    
    def __init__(self, agent: RealtimeVoiceAgent):
        """
        Initialize interface
        
        Args:
            agent: Initialized RealtimeVoiceAgent
        """
        self.agent = agent
        self.is_recording = False
        self.conversation_active = False
        
    async def start_session(self):
        """Start a new conversation session"""
        if not self.conversation_active:
            logger.info("ğŸ¬ Starting new session")
            self.conversation_active = True
            # Reset conversation history
            if self.agent.conversation_manager:
                self.agent.conversation_manager.conversation_history.clear()
            return "âœ… Session dÃ©marrÃ©e - Vous pouvez maintenant parler"
        return "âš ï¸ Session dÃ©jÃ  active"
    
    async def stop_session(self):
        """Stop current conversation session"""
        if self.conversation_active:
            logger.info("â¹ï¸ Stopping session")
            self.conversation_active = False
            await self.agent.stop_conversation()
            return "âœ… Session terminÃ©e"
        return "âš ï¸ Aucune session active"
    
    async def process_audio_input(
        self,
        audio_input: Optional[Tuple[int, np.ndarray]]
    ) -> Tuple[str, Optional[Tuple[int, np.ndarray]]]:
        """
        Process audio input from microphone
        
        Args:
            audio_input: Tuple of (sample_rate, audio_array) from Gradio Audio
            
        Returns:
            Tuple of (status_message, audio_output)
        """
        if not self.conversation_active:
            return "âš ï¸ Session non active - Cliquez sur 'DÃ©marrer Session'", None
        
        if audio_input is None:
            return "âš ï¸ Aucun audio dÃ©tectÃ©", None
        
        start_time = time.time()
        
        try:
            sample_rate, audio_data = audio_input
            
            logger.info(f"ğŸ¤ Audio reÃ§u: {len(audio_data)} samples @ {sample_rate}Hz")
            
            # Convert to int16 PCM bytes
            if audio_data.dtype != np.int16:
                audio_data = (audio_data * 32767).astype(np.int16)
            
            audio_bytes = audio_data.tobytes()
            
            # Process through pipeline
            await self.agent.process_audio_chunk(audio_bytes, sample_rate)
            
            # Wait a bit for processing
            await asyncio.sleep(1.0)
            
            # Get response audio
            response_audio = self.agent.audio_collector.get_audio()
            
            if response_audio and len(response_audio) > 0:
                # Convert bytes to numpy array
                audio_array = np.frombuffer(response_audio, dtype=np.int16)
                output_audio = (22050, audio_array)
                
                elapsed = time.time() - start_time
                status = f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e en {elapsed:.1f}s ({len(response_audio)} bytes)"
                
                # Clear audio collector for next turn
                self.agent.audio_collector.clear()
                
                return status, output_audio
            else:
                return "âš ï¸ Aucune rÃ©ponse audio gÃ©nÃ©rÃ©e", None
                
        except Exception as e:
            logger.error(f"âŒ Error processing audio: {e}", exc_info=True)
            return f"âŒ Erreur: {str(e)}", None
    
    async def process_text_input(self, text_input: str) -> Tuple[str, str, Optional[Tuple[int, np.ndarray]]]:
        """
        Process text input (alternative to voice)
        
        Args:
            text_input: User question as text
            
        Returns:
            Tuple of (subject, response_text, audio_output)
        """
        if not self.conversation_active:
            return "â“ Non dÃ©tectÃ©", "âš ï¸ Session non active", None
        
        if not text_input or text_input.strip() == "":
            return "â“ Non dÃ©tectÃ©", "âš ï¸ Entrez une question", None
        
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ“ Text input: {text_input}")
            
            # Get RAG context
            subject, context = self.agent.rag_service.retrieve(text_input)
            logger.info(f"ğŸ“š Subject: {subject}")
            
            # Build prompt
            system_prompt = f"""Tu es un tuteur IA spÃ©cialisÃ© en {subject}.
Utilise le contexte suivant pour rÃ©pondre de maniÃ¨re prÃ©cise et pÃ©dagogique.

Contexte:
{context}

RÃ©ponds de maniÃ¨re claire et concise (2-3 phrases maximum).
N'utilise pas de caractÃ¨res spÃ©ciaux car ta rÃ©ponse sera convertie en audio."""
            
            # Get LLM response
            response_text = await self.agent.llm_service.generate_response(
                prompt=text_input,
                system_prompt=system_prompt
            )
            
            logger.info(f"ğŸ¤– Response: {response_text[:100]}...")
            
            # Generate audio
            audio_bytes = await self.agent.tts_service.synthesize(response_text)
            
            # Prepare output
            subject_emoji = {
                'maths': 'ğŸ”¢ MathÃ©matiques',
                'physique': 'âš›ï¸ Physique',
                'anglais': 'ğŸ‡¬ğŸ‡§ Anglais',
                'unknown': 'â“ Non dÃ©tectÃ©'
            }.get(subject, f'ğŸ“š {subject}')
            
            audio_output = None
            if audio_bytes and len(audio_bytes) > 0:
                audio_array = np.frombuffer(audio_bytes, dtype=np.int16)
                audio_output = (22050, audio_array)
            
            elapsed = time.time() - start_time
            response_with_info = f"{response_text}\n\nâ±ï¸ Temps: {elapsed:.1f}s"
            
            return subject_emoji, response_with_info, audio_output
            
        except Exception as e:
            logger.error(f"âŒ Error: {e}", exc_info=True)
            return "âŒ Erreur", f"Erreur: {str(e)}", None
    
    def create_interface(self) -> gr.Blocks:
        """
        Create Gradio interface
        
        Returns:
            Gradio Blocks interface
        """
        with gr.Blocks(title="Agent Vocal IA Local - Temps RÃ©el") as interface:
            gr.Markdown("""
            # ğŸ™ï¸ Agent Vocal IA Local - Temps RÃ©el avec RAG
            
            **Mode Temps RÃ©el** : Conversation continue jusqu'Ã  dÃ©connexion
            
            ### ğŸš€ Comment utiliser:
            1. Cliquez sur **"DÃ©marrer Session"**
            2. Parlez via le micro OU tapez votre question
            3. L'IA rÃ©pond avec contexte RAG (maths/physique/anglais)
            4. Cliquez sur **"ArrÃªter Session"** pour terminer
            
            ---
            """)
            
            with gr.Row():
                with gr.Column():
                    session_status = gr.Textbox(
                        label="ğŸ“Š Statut Session",
                        value="âšª Session non dÃ©marrÃ©e",
                        interactive=False
                    )
                    
                    with gr.Row():
                        start_btn = gr.Button("â–¶ï¸ DÃ©marrer Session", variant="primary")
                        stop_btn = gr.Button("â¹ï¸ ArrÃªter Session", variant="stop")
            
            gr.Markdown("### ğŸ¤ Mode Audio (Micro)")
            
            with gr.Row():
                with gr.Column():
                    audio_input = gr.Audio(
                        sources=["microphone"],
                        type="numpy",
                        label="ğŸ¤ Parlez ici",
                        streaming=False
                    )
                    audio_process_btn = gr.Button("ğŸµ Traiter Audio")
                
                with gr.Column():
                    audio_output = gr.Audio(
                        label="ğŸ”Š RÃ©ponse Audio",
                        type="numpy"
                    )
                    audio_status = gr.Textbox(
                        label="ğŸ“Š Statut",
                        interactive=False
                    )
            
            gr.Markdown("### ğŸ’¬ Mode Texte (Alternative)")
            
            with gr.Row():
                with gr.Column():
                    text_input = gr.Textbox(
                        label="ğŸ“ Votre question",
                        placeholder="Ex: Comment rÃ©soudre une Ã©quation du second degrÃ© ?",
                        lines=3
                    )
                    text_submit_btn = gr.Button("ğŸ“¤ Envoyer")
                
                with gr.Column():
                    subject_output = gr.Textbox(
                        label="ğŸ“š Domaine dÃ©tectÃ©",
                        interactive=False
                    )
                    response_output = gr.Textbox(
                        label="ğŸ’¡ RÃ©ponse",
                        lines=8,
                        interactive=False
                    )
                    text_audio_output = gr.Audio(
                        label="ğŸ”Š Audio de la rÃ©ponse",
                        type="numpy"
                    )
            
            # Event handlers
            start_btn.click(
                fn=self.start_session,
                outputs=session_status
            )
            
            stop_btn.click(
                fn=self.stop_session,
                outputs=session_status
            )
            
            audio_process_btn.click(
                fn=self.process_audio_input,
                inputs=audio_input,
                outputs=[audio_status, audio_output]
            )
            
            text_submit_btn.click(
                fn=self.process_text_input,
                inputs=text_input,
                outputs=[subject_output, response_output, text_audio_output]
            )
        
        return interface


async def launch_gradio_realtime(
    whisper_model: str = "base",
    ollama_model: str = "qwen2:1.5b",
    device: str = "cuda",
    share: bool = False
):
    """
    Launch Gradio interface for realtime voice agent
    
    Args:
        whisper_model: Whisper model size
        ollama_model: Ollama model name
        device: Device (cuda/cpu)
        share: Whether to create public link
    """
    logger.info("ğŸ¨ Launching Gradio Realtime Interface...")
    
    # Create and initialize agent
    agent = await create_realtime_voice_agent(
        whisper_model=whisper_model,
        ollama_model=ollama_model,
        device=device
    )
    
    # Create interface
    ui = GradioRealtimeInterface(agent)
    interface = ui.create_interface()
    
    # Launch
    logger.info("ğŸš€ Gradio interface ready!")
    interface.launch(
        share=share,
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=False
    )


if __name__ == "__main__":
    import sys
    
    # Parse args
    device = "cuda" if "--cpu" not in sys.argv else "cpu"
    share = "--share" in sys.argv
    
    # Run
    asyncio.run(launch_gradio_realtime(
        whisper_model="base",
        ollama_model="qwen2:1.5b",
        device=device,
        share=share
    ))
